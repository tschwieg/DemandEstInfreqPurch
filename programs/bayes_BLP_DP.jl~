using LinearAlgebra
using Distributions
using ForwardDiff
using GLM
using Printf
using Dates

# Notation here is: Parameter -> Hyper-parmeter in Camel-Case
# So lambdaAlpha is the alpha-prior for the lambda parameter
struct PriorBLP
    lambdaAlpha::Float64
    lambdaTheta::Float64
    betaMean::Vector{Float64}
    betaVarInv::Matrix{Float64}
    gammaDiagMu::Float64
    gammaDiagSigma::Float64
    gammaOffDiagMu::Float64
    gammaOffDiagSigma::Float64
    etaMean::Vector{Float64}
    etaVarInv::Matrix{Float64}
    ## These are DP hyper-parameters directly from Rossi (2013)
    muBar::Vector{Float64}
    aMu::Float64
    alpha::Float64
end

struct SearchParameters
    sigmaVarDiag::Float64
    sigmaVarOffDiag::Float64
    dScale::Float64
    lambdaTVar::Float64
    lambdaDVar::Float64
    gammaVar::Float64
    alphaVar::Float64
end

mutable struct Utility
    # This is the denominator in the predict shares, faster not to have to allocate the memory
    denom::Vector{Float64}
    # This contains
    shareHolder::Vector{Float64}
    # This contains solutions to the market inversion, before we have decided to accept the draw.
    solutionFill::Matrix{Float64}
    # This contains the likelihood of the previously accepted market
    LMarket::Vector{Float64}
    # This contains the likelihood of the last inverted market, which
    # may or may not be accepted and put into LMarket.
    LMarketTemp::Vector{Float64}
    # Jac[i] is an i x i matrix containing the jacobian for markets of that size
    Jac::Vector{Matrix{Float64}}
    # sJac is a filler used for the shares in construction the
    # jacobian. Don't want to reallocate memory each time.
    sJac::Vector{Matrix{Float64}}
    # δ contains the steps in Newton's method as we invert the system.
    δ::Vector{Float64}
    # This is the δ values for the last accepted inversion. This gives
    # good initialization to Newton's method.
    warmStart::Array{Float64,2}
    # This is the logabsdet of the last accepted share draw -- saves computing two logabsdets. 
    shareJacLDet::Vector{Float64}
end

struct Data
    X::Vector{Matrix{Float64}}
    q::Array{Int64,2}
    Z::Vector{Matrix{Float64}}
    J::Vector{Int64}
    T::Int64
    searches::Vector{Int64}
    N::Int64
    K::Int64
    ## The first L of the K parameters have random coefficients
    L::Int64
    numInst::Int64
    cMap::Array{Int64,2}
end

function IndPredictShares( δ::Vector{<:Real} )
    vMax = maximum(δ)

    num = exp.(δ .- vMax)
    return num ./ (exp(-vMax) + sum(num))
end

function PredictSharesBLP( δ::Vector{<:Real},  X::Matrix{<:Real}, I::Int64,
                        Γ::Matrix{<:Real}, ζ::Matrix{<:Real})
    return mean( IndPredictShares( δ +  X * (Γ * ζ[i,:])) for i in 1:I)
end

function PredictSharesBCS( δ::Vector{<:Real},  X::Matrix{<:Real},
                           γ::Float64, α::Float64)
    return γ*IndPredictShares( δ ) + (1.0-γ)*IndPredictShares( δ + α*X[:,1] )
end

function IndBuildJac( s::Vector{<:Real}, J::Int64 )
    Jac = Matrix{Real}(undef,J,J)#zeros(J,J)
    Jac .= 0.0
    for j in 1:J
        for k in 1:J
            if j == k
                Jac[j,j] = s[j]*(1-s[j])
            else
                Jac[j,k] = -s[j]*s[k]
            end
        end
    end
    return Jac
end

function BuildXiJacBCS( X::Matrix{<:Real}, δ::Vector{<:Real},
                        γ::Float64, α::Float64, J::Int64)

    return γ*IndBuildJac(IndPredictShares( δ ),J) +
        (1.0-γ)*IndBuildJac(IndPredictShares( δ + α*X[:,1] ),J)
end

function BuildXiJacBLP( X::Matrix{<:Real}, δ::Vector{<:Real}, nSumers::Int64,
                     Γ::Matrix{<:Real}, ζ::Matrix{<:Real}, J::Int64)
    return mean( IndBuildJac( IndPredictShares( δ +  X * (Γ * ζ[i,:])), J )
                  for i in 1:nSumers)
end

function BuildPriceJacBLP( X::Matrix{<:Real}, δ::Vector{<:Real}, nSumers::Int64,
                     Γ::Matrix{<:Real}, ζ::Matrix{<:Real}, J::Int64, β::Vector{Float64})
    return mean( (β[1] + (Γ * ζ[i,:])[1]) .*
                 IndBuildJac( IndPredictShares( δ +  X * (Γ * ζ[i,:])), J )
                  for i in 1:nSumers)
end

function GInvertShares( X::Matrix{<:Real}, s::Vector{<:Real}, nSumers::Int64,
                        Γ::Matrix{<:Real}, ζ::Matrix{<:Real}, J::Int64,
                        delPrev::Vector{<:Real})

    delPrev = zeros(J)
    sHat = log.(PredictSharesBLP( delPrev, X, nSumers, Γ, ζ))
    
    sTrue = log.(s)
    diff = 1e5
    #This doesn't need to be so tight
    while( diff > 1e-3)
        delNew = delPrev + sTrue - sHat
        diff = maximum( abs.( delNew - delPrev))
        
        delPrev = delNew
        sHat = log.(PredictSharesBLP( delPrev, X, nSumers, Γ, ζ))
    end

    #Now we do a Newton Step. Need the log-jacobian.
    #Since its the log jacobian we divide by PredictShares() which is exp( sHat)
    # sHat = log.(PredictShares( delPrev, X, I, Γ, ζ))
    # sTrue = log.(s)
    # diff = 1e5

    ## Upgrade to Werner-King?
    sJac = BuildXiJacBLP( X, delPrev, nSumers, Γ, ζ, J ) ./ exp.( sHat )
    sHat = log.(PredictSharesBLP( delPrev, X, nSumers, Γ, ζ))
    while( diff > 1e-14)
        delNew = delPrev - sJac \ (sHat - sTrue)
        diff = maximum( abs.( sHat - sTrue))
        #println( diff)
        delPrev = delNew
        sJac = BuildXiJacBLP( X, delPrev, nSumers, Γ, ζ, J ) ./ exp.( sHat )
        sHat = log.(PredictSharesBLP( delPrev, X, nSumers, Γ, ζ))
    end
    #println(diff)
    return delPrev
end

function FastInvertShares( X::Matrix{<:Real}, s::Vector{<:Real}, nSumers::Int64,
                           Γ::Matrix{<:Real}, ζ::Matrix{<:Real}, J::Int64,
                           delPrev::Vector{<:Real})


    #Now we do a Newton Step. Need the log-jacobian.
    #Since its the log jacobian we divide by PredictShares() which is exp( sHat)
    sHat = log.(PredictSharesBLP( delPrev, X, nSumers, Γ, ζ))
    sTrue = log.(s)
    diff = maximum( abs.( sHat - sTrue))

    #This doesn't need to be so tight
    while( diff > 1e-1)
        delNew = delPrev + sTrue - sHat
        diff = maximum( abs.( delNew - delPrev))
        
        delPrev = delNew
        sHat = log.(PredictSharesBLP( delPrev, X, nSumers, Γ, ζ))
    end

    ## Upgrade to Werner-King

    its = 0
    Xn = delPrev
    Yn = delPrev

    #sJac = diagm(ones(data.K))
    XnNew = delPrev

    while( its < 50)
        wernerPoint = .5*Xn + .5*Yn
        sJac = BuildXiJacBLP( X, wernerPoint, nSumers, Γ, ζ, J  ) ./
            PredictSharesBLP( wernerPoint, X, nSumers, Γ, ζ)

        if logabsdet( sJac )[1] < -20 || any( isnan.(sJac))
            #println("Werner-King Det Problems")
            return ones(J)*NaN
            #return GInvertShares( X, s, nSumers, Γ, ζ, J, delPrev)
        end
        
        XnNew = Xn - sJac \ (sHat - sTrue)

        sHat = log.(PredictSharesBLP( XnNew, X, nSumers, Γ, ζ))

        diff = maximum( abs.( sHat - sTrue))
        #println( diff)
        if diff <= 1e-12
            delPrev = Xn
            break
        end

        #sJac = BuildXiJac( X, delPrev, nSumers, Γ, ζ, J ) ./ exp.( sHat )
        
        Yn = XnNew - sJac \ (sHat - sTrue)

        Xn = XnNew
        sHat = log.(PredictSharesBLP( Xn, X, nSumers, Γ, ζ))

        its += 1
    end

    if its >= 50
        #println("Werner-King Problems")
        return ones(J)*NaN
        #return GInvertShares( X, s, nSumers, Γ, ζ, J, delPrev)
    end
    

    return delPrev
end




## Overloaded version for BCS
function FastInvertShares( X::Matrix{<:Real}, s::Vector{<:Real}, 
                           γ::Float64, α::Float64, J::Int64,
                           delPrev::Vector{<:Real})


    #Now we do a Newton Step. Need the log-jacobian.
    #Since its the log jacobian we divide by PredictShares() which is exp( sHat)
    sHat = log.(PredictSharesBCS( delPrev, X, γ, α))
    sTrue = log.(s)
    diff = 1e5

    ## Upgrade to Werner-King

    its = 0
    Xn = delPrev
    Yn = delPrev
    
    
    
    while( its < 75)
        wernerPoint = .5*Xn + .5*Yn
        sJac = BuildXiJac( X, wernerPoint, nSumers, Γ, ζ, J ) ./
            PredictSharesBCS( delPrev, X, γ, α)
        sHat = log.(PredictSharesBCS( delPrev, X, γ, α))

        XnNew = Xn - sJac \ (sHat - sTrue)
        diff = maximum( abs.( sHat - sTrue))
        #println( diff)
        if diff <= 1e-12
            delPrev = Xn
            break
        end

        #sJac = BuildXiJac( X, delPrev, nSumers, Γ, ζ, J ) ./ exp.( sHat )
        sHat = log.(PredictSharesBCS( delPrev, X, γ, α))
        
        Yn = XnNew - sJac \ (sHat - sTrue)
        diff = maximum( abs.( sHat - sTrue))
        #println( diff)
         if diff <= 1e-12
            delPrev = Yn
            break
        end
        Xn = XnNew
        its += 1
    end
    if its >= 75
        println("Werner-King Problems")
        println( sHat)
        println( sTrue)
        println(diff)
        println(delPrev)
        @assert( 1 == 0)
    end
    

    return delPrev
end


## MarketLikelihood()
## Given a set of xi, and the distribution of xi, return the likelihood of ξ
function MarketLikelihood( xi::Vector{Float64}, exV::Vector{Float64}, sd::Vector{Float64},
                           J::Int64, Jac::Matrix{Float64})

    if any( isnan.( xi))
        #println(usedContraction, " ", δ)
        return -1e20
    end
    ℓ = 0.0
    for j in 1:J
        ℓ += logpdf( Normal(exV[j], sd[j]), xi[j])
    end
    ℓ -= logabsdet( Jac )[1]
    return ℓ
end


function DrawLambda( data::Data, lambT::Vector{Float64},
                     newLamT::Vector{Float64}, s::Array{Float64,2},
                     pars::SearchParameters, lamMap::Vector{Vector{Int64}},
                     priors::PriorBLP)
    # For a given market, the λ is given by both the DFD and DD Fixed
    # Effect. We need to write a poisson regression to handle this.

    # Any market (t,d) has an arrival rate of exp( lambT[t] + lambD[d] )
    # function LikelihoodT(l,t)
    #     ℓ = 0.0
    #     ℓ += logpdf( Poisson(exp(l)), data.searches[t])
    #     for j in 1:data.J[t]
    #         ℓ += logpdf( Poisson( exp(l)*s[t,j]),
    #                      data.q[t,j] )

    #     end
    #     return ℓ
    # end
    # coinFlips = log.(rand( Uniform(), data.T))

    for (i,tCol) in enumerate( lamMap )

        postK = sum( sum( data.q[t,1:data.J[t]]) + data.searches[t] for t in tCol) +
            priors.lambdaAlpha

        postTheta = priors.lambdaTheta / ( priors.lambdaTheta*sum( 2.0 - s[t,data.J[t]+1]
                                                                   for t in tCol) + 1)
        newLamT[tCol] .= log(rand( Gamma( postK, postTheta)))
            

        
        # candLam = lambT[tCol[1]] + rand(Normal(0.0,pars.lambdaTVar))

        # baseLik = sum( LikelihoodT( lambT[t], t) for t in tCol )

        # newLik = sum( LikelihoodT( candLam, t) for t in tCol )

        # priorOld = logpdf( Gamma(priors.lambdaAlpha, priors.lambdaTheta), exp(lambT[tCol[1]]))
        # priorNew = logpdf( Gamma(priors.lambdaAlpha, priors.lambdaTheta), exp(candLam))

        # if coinFlips[i] <= newLik - baseLik + priorNew - priorOld
        #     newLamT[tCol] .= candLam
        # else
        #     newLamT[tCol] = lambT[tCol]
        # end
    end
    return newLamT
end


function DrawShares( data::Data, s::Matrix{Float64}, lambT::Vector{Float64},
                     β::Vector{Float64},
                     util::Utility, newShare::Matrix{Float64},
                     pars::SearchParameters, 
                     exV::Vector{Float64}, sd::Vector{Float64},
                     cMap::Matrix{Int64}, nSumers::Int64, Γ::Matrix{Float64},
                     γ::Float64, α::Float64, BLP::Bool,
                     ζ::Matrix{Float64})

    coinFlips = log.(rand(Uniform(), data.T));

    tempShare = zeros(maximum(data.J));
    
    #The posterior likelihood of a set of shares is the poisson
    #likelihood times the prior probability coming from the inversion
    for t in 1:data.T

        if data.J[t] == 0
            continue
        end

        J = data.J[t]

        noiseAdd =  rand(Normal(0.0, pars.dScale), J)

        ## Since we know the δ, we don't need to do an inversion to get it. 
        δ = util.warmStart[t,1:J] + noiseAdd

        logdetOld = util.shareJacLDet[t]



        if BLP
            util.Jac[t] = BuildXiJacBLP( data.X[t], δ, nSumers, Γ, ζ, J )
            tempShare = PredictSharesBLP( δ, data.X[t], nSumers, Γ, ζ )
        else
            util.Jac[t] = BuildXiJacBCS( data.X[t], δ, γ, α, J )
            tempShare = PredictSharesBCS( δ, data.X[t], γ, α )
        end
        logdetNew = logabsdet(util.Jac[t])[1]




        ## Is this a problem? I guess this makes the searches
        ## unbalanced and we need to control for this in the M-H
        ## step. This is effectively a uniform prior so we shouldn't have to though. 
        if min( minimum( tempShare[1:J]),
                1.0 - sum(tempShare[1:J])) < 1e-12
            newShare[t,1:(J+1)] = s[t,1:(J+1)]
            continue
        end

        
        λ = exp( lambT[t])

        qLik = 0.0
        qLikOld = 0.0
        for j in 1:J
            qLik += logpdf( Poisson( λ*tempShare[j]), data.q[t,j] )
            qLikOld += logpdf( Poisson( λ*s[t,j]), data.q[t,j] )
        end

        ## We predict shares based on δ, but likelihood uses ξ
        newLik = MarketLikelihood( δ - data.X[t]*β, exV[cMap[t,1:J]], sd[cMap[t,1:J]],
                                   J, util.Jac[t])

        if coinFlips[t] <= (newLik + qLik - util.LMarket[t] - qLikOld  +
                              logdetNew - logdetOld)

            
            util.warmStart[t,1:J] = δ#util.solutionFill[t,d,1:J]
            util.LMarket[t] = newLik
            util.shareJacLDet[t] = logdetNew
            newShare[t,1:J] = tempShare[1:J]
            newShare[t,J+1] = 1.0 - sum( tempShare[1:J] )
            #println("passed Draw for t = $t")
        else
            #println("failed Draw for t = $t")
            newShare[t,1:(J+1)] = s[t,1:(J+1)]
        end
    end
    return newShare
end


## Nothing here is specific to β, runs for η as well.
function DrawBayesReg(data::Data, yVec::Matrix{Float64},
                      deltaVec::Vector{Float64}, util::Utility,
                      xHatMod::Vector{Float64},
                      exV::Vector{Float64}, sd::Vector{Float64},
                      cMap::Matrix{Int64}, xHat::Matrix{Float64},
                      priorMean, priorVarInv, K::Int64)


    counter = 1
    for t in 1:data.T
        for j in 1:data.J[t]
            xHatMod[counter] = 1.0 / (sd[cMap[t,j]])

            # global counter
            deltaVec[counter] = (yVec[t,j] -
                                 exV[counter])*xHatMod[counter]
            counter += 1
        end
    end

    xHat = xHat .* xHatMod;

    #Since we "observe" υ, that is information about ξ that needs to be used.
    #sd = sqrt(Σ[2,2] - (Σ[1,2]*Σ[1,2] / Σ[1,1]))

    betaTilde = (xHat'*xHat + priorVarInv) \
        (xHat'*deltaVec + priorVarInv*priorMean)

    varDraw = Hermitian(inv(xHat'*xHat + priorVarInv));
    F = cholesky(varDraw)

    betaDraw = betaTilde + F.L*rand( Normal(), K)

    return betaDraw
end

function DrawGammaBCS(data::Data, util::Utility,
                      priors, γ::Float64, α::Float64,
                      exV::Vector{Float64}, sd::Vector{Float64},
                      cMap::Matrix{Int64}, s::Matrix{Float64},
                      pars::SearchParameters)

    candGam = γ + rand( Normal(0.0, pars.gammaVar))

    if candGam < 1e-8 || candGam > 1.0-1e-8
        newGam = γ
        return newGam
    end
    
    
    ## Invert shares with new gamma. Continue as if α before. 
    for t in 1:data.T
        J = data.J[t]

        util.solutionFill[t,1:J] = FastInvertShares( data.X[t], s[t,1:J], 
                                                  candGam, α, J,
                                                  util.warmStart[t,1:J])
        util.Jac[t] = BuildXiJacBCS(data.X[t], util.solutionFill[t,1:J],
                                    candGam, α, J)
        
        ξ = util.solutionFill[t,1:J] - data.X[t]*β

        util.LMarketTemp[t] = MarketLikelihood( ξ, exV[cMap[t,1:J]], sd[cMap[t,1:J]],
                                                J, util.Jac[t])
    end
    newLik = sum( util.LMarketTemp[t] for t in 1:data.T)
    oldLik = sum( util.LMarket[t] for t in 1:data.T)
    if coinFlip < newLik - oldLik
        newGam = candGam
        for t in 1:data.T
            util.LMarket[t] = util.LMarketTemp[t]
            for j in 1:data.J[t]
                util.warmStart[t,j] = util.solutionFill[t,j]
            end
        end
    else
        newGam = γ
    end
    return newGam
end

function DrawAlphaBCS(data::Data, util::Utility,
                      priors, γ::Float64, α::Float64,
                      exV::Vector{Float64}, sd::Vector{Float64},
                      cMap::Matrix{Int64}, s::Matrix{Float64},
                      pars::SearchParameters)

    candAlpha = α + rand( Normal(0.0, pars.alphaVar))

    ## Prior will throw a fit anyways, but lets make sure we don't
    ## have out of domain sillyness.
    if candAlpha < 1e-8 
        newAlpha = α
        return newAlpha
    end
    
    
    ## Invert shares with new gamma. Continue as if α before. 
    for t in 1:data.T
        J = data.J[t]

        util.solutionFill[t,1:J] = FastInvertShares( data.X[t], s[t,1:J], 
                                                     γ, candAlpha, J,
                                                     util.warmStart[t,1:J])
        util.Jac[t] = BuildXiJacBCS(data.X[t], util.solutionFill[t,1:J],
                                    γ, candAlpha, J)
        
        ξ = util.solutionFill[t,1:J] - data.X[t]*β

        util.LMarketTemp[t] = MarketLikelihood( ξ, exV[cMap[t,1:J]], sd[cMap[t,1:J]],
                                                J, util.Jac[t])
    end
    newLik = sum( util.LMarketTemp[t] for t in 1:data.T)
    oldLik = sum( util.LMarket[t] for t in 1:data.T)

    newLik += logpdf( LogNormal( priors.alphaEXV, priors.alphaVar), -candAlpha )
    oldLik += logpdf( LogNormal( priors.alphaEXV, priors.alphaVar), -α )
    
    if coinFlip < newLik - oldLik
        newGam = candGam
        for t in 1:data.T
            util.LMarket[t] = util.LMarketTemp[t]
            for j in 1:data.J[t]
                util.warmStart[t,j] = util.solutionFill[t,j]
            end
        end
    else
        newGam = γ
    end
    return newGam
end



function DrawGammaBLP(data::Data, util::Utility,
                      priors::PriorBLP, Γ::Matrix{Float64},
                      exV::Vector{Float64}, sd::Vector{Float64},
                      cMap::Matrix{Int64}, s::Matrix{Float64},
                      nSumers::Int64, pars::SearchParameters,
                      beta::Vector{Float64}, ζ)
    
    coinFlip = log(rand(Uniform()))
    ## The first L elements are non-zero
    oldGam = cholesky(Hermitian(Γ[1:data.L,1:data.L])).U

    candGamChol = zeros(data.L,data.L)
    for k in 1:data.L
        ## Diagonal elements of a cholesky U must be positive. Parameterize this.
        candGamChol[k,k] = exp(log(oldGam[k,k]) +
                               rand( Uniform(-pars.sigmaVarDiag,pars.sigmaVarDiag)))
        if pars.sigmaVarOffDiag > 0.0
            for j in (k+1):data.L
                candGamChol[k,j] = oldGam[k,j] + rand( Uniform(-pars.sigmaVarOffDiag,
                                                               pars.sigmaVarOffDiag))
            end
        end
        
    end

    ## This allows for arbitrary covariance between RC parameters,
    ## while also having terms remain in the B-H δ
    candGam = zeros(data.K,data.K)
    candGam[1:data.L,1:data.L] = candGamChol'*candGamChol

    ## Invert shares with new gamma. Continue as if α before. 
    for t in 1:data.T
        J = data.J[t]

        util.solutionFill[t,1:J] = FastInvertShares( data.X[t], s[t,1:J], nSumers,
                                                     candGam, ζ, J,
                                                     util.warmStart[t,1:J])
        
        util.Jac[t] = BuildXiJacBLP(data.X[t], util.solutionFill[t,1:J],
                                    nSumers, candGam, ζ, J)
        
        ξ = util.solutionFill[t,1:J] - data.X[t]*beta

        util.LMarketTemp[t] = MarketLikelihood( ξ, exV[cMap[t,1:J]], sd[cMap[t,1:J]],
                                                J, util.Jac[t])
    end

    newLik = sum( util.LMarketTemp[t] for t in 1:data.T)
    oldLik = sum( util.LMarket[t] for t in 1:data.T)

    ## In addition to priors, our choice distribution q for the
    ## diagonals is not reversible. Use change of variable theorem here.
    for l in 1:data.L
        ## We don't need to worry about the support conditions for
        ## each, since the band never changes so both are within each
        ## other's support.
        newLik += -log( oldGam[l,l] )
        oldLik += -log( candGamChol[l,l])
    end
    
    

    for l in 1:data.L
        newLik += logpdf( Gamma( priors.gammaDiagMu, priors.gammaDiagSigma),
                          candGamChol[l,l])
        oldLik += logpdf( Gamma( priors.gammaDiagMu, priors.gammaDiagSigma),
                          oldGam[l,l])
        for k in (l+1):data.L
            newLik += logpdf( Normal( priors.gammaOffDiagMu, priors.gammaOffDiagSigma),
                              candGamChol[l,k])
            oldLik += logpdf( Normal( priors.gammaOffDiagMu, priors.gammaOffDiagSigma),
                              oldGam[l,k])
        end
    end
    
    
    
    if coinFlip < newLik - oldLik
        #println("Gam Accepted")
        newGam = candGam
        for t in 1:data.T
            util.LMarket[t] = util.LMarketTemp[t]
            for j in 1:data.J[t]
                util.warmStart[t,j] = util.solutionFill[t,j]
            end
        end
    else
        newGam = Γ
    end
    return newGam
end

function DoMultivariateReg(X::Matrix{Float64}, A::Matrix{Float64},
                           muBar::Vector{Float64}, Y::Matrix{Float64},
                           nu::Float64, V::Matrix{Float64})
    BTilde = (X'*X + A) \ (X'*Y + A*muBar')
    S = (Y - X'BTilde)'*(Y - X'*BTilde) + (BTilde - muBar')'*A*(BTilde - muBar')
            
    Σ = rand(InverseWishart(nu+1, Matrix(Hermitian(nu*V + S))))
    μ = rand(MvNormal(reshape(BTilde, (2)), Symmetric(Σ .* inv(X'*X + A))))

    return (Σ,μ)
end



function DrawVMapDP(α::Float64, sigmaSize::Int64, N::Int64,
                    μ::Matrix{Float64}, Σ::Array{Float64,3},
                    upsilon::Vector{Float64}, xi::Vector{Float64},
                    vMap::Array{Int64}, V::Matrix{Float64}, nu::Float64,
                    aMu::Float64, muBar::Vector{Float64}, maxSigma::Int64)
    
    ## Get the counts for each nJ
    nJ = zeros(N);
    for n in 1:N
        nJ[vMap[n]] += 1
    end

    dat = hcat( upsilon, xi);

    R = cholesky(V).U
    logdetR = log( R[1,1] + R[2,2] )

    aMuFrac = aMu / (1.0 + aMu)

    ## R is already upper triangular cholesky so inv() isn't a bad call
    m = sqrt( aMuFrac )*inv(R)*( dat' .- muBar);

    vivi = [sum( m[:,k].^2 ) for k in 1:size(m,2)];

    base = log( (nu - 1) / 2.0) + log( aMuFrac ) - log( π ) - logdetR

    lnq0v = base .- ((nu-1) / 2.0) .* log.( 1.0 .+ vivi)
    q0 = exp.(lnq0v);

    ## Maximum possible size of sigmaSize is data.N
    probHolder = zeros(N+1);

    #α = 5.0

    for n in 1:N
        probHolder .= 0.0
        #println(n)
        #global sigmaSize
        datN = dat[n,:]
        for i in 1:sigmaSize
            probHolder[i] = nJ[i]*pdf( MvNormal( μ[i,1:2], Σ[i,:,:]), datN)# / (α + N - 1)
        end
        probHolder[sigmaSize+1] = q0[n]*(α )# / (α + N - 1)

        ## Cap the number of possible distributions.
        if sigmaSize == maxSigma
            probHolder[sigmaSize+1] = 0.0
        end
        

        # while( sum( probHolder ) != 1.0)

        probHolder ./= sum(probHolder)
        #normalize!( probHolder, 1)


        oldVMap = vMap[n]
        fullCheck = nJ[oldVMap]

        vMapUnif = rand(Uniform())
        cumSumPHolder = cumsum( probHolder)
        vMap[n] = 1
        for i in 1:length(probHolder)
            if vMapUnif < cumSumPHolder[i]
                vMap[n] = i
                break
            end
        end
        
        #vMap[n] = rand( Categorical( probHolder))


        oldSigmaSize = sigmaSize
        
        if vMap[n] == sigmaSize + 1

            ## Do a multivariate regression to get posterior means and variances conditional on this single data point, and our priors. 
            X = ones(1,1)
            A = ones(1,1)*aMu

            
            #rand(MvNormal(BTilde,(1.0 / (aMu+1))*Σ[vMap[n],:,:]))
            
            ## If we were the only data point, then we replace that
            ## distribution, and don't change the total number
            if fullCheck == 1.0
                vMap[n] = oldVMap
                Σ[vMap[n],:,:],μ[vMap[n],:] = DoMultivariateReg(X, A,
                                                            muBar, dat[n:n,1:2],
                                                            nu, V)
            else
                Σ[vMap[n],:,:],μ[vMap[n],:] = DoMultivariateReg(X, A,
                                                                muBar, dat[n:n,1:2],
                                                                nu, V)
                ## Otherwise we've added a new distribution, draw its stuff.
                nJ[oldVMap] -= 1
                nJ[sigmaSize+1] = 1
                sigmaSize += 1
            end
        elseif fullCheck == 1.0 && oldVMap != vMap[n]
            if oldVMap == sigmaSize
                nJ[oldVMap] = 0.0
                nJ[vMap[n]] += 1
                
                sigmaSize -= 1
            else## There is at least one occupied cluster with greater index
                ## If we are the only data point and switch, switch
                ## oldVMap with sigmaSize and decrement sigmaSize
                # if sigmaSize == 1
                #     println("Abort Abort")
                #     println(nJ[oldVMap])
                #     println(oldVMap)
                #     println(vMap[n])
                #     println(sigmaSize)
                #     @assert( 12 == 0 )
                # end
                
                
                nJ[oldVMap] = nJ[sigmaSize]
                nJ[sigmaSize] = 0.0
                μ[oldVMap,:] = μ[sigmaSize,:]
                Σ[oldVMap,:,:] = Σ[sigmaSize,:,:]
                vMap[findall(x->x==sigmaSize,vMap)] .= oldVMap
                nJ[vMap[n]] += 1
                sigmaSize -= 1
            end
        else
            nJ[oldVMap] -= 1
            nJ[vMap[n]] += 1
        end
        if sigmaSize != maximum( vMap )
            println("Big Problem Abort")
            println(nJ[oldVMap])
            println(oldVMap)
            println(vMap[n])
            println(sigmaSize)
            println(oldSigmaSize)
            @assert( 12 == 0 )
        end
        
    end
    return vMap,μ,Σ,sigmaSize#,π
end

function DrawSigma( upsilon, xi, sigmaSize, vMap,
                    muBar, aMu, sdXi, exVXi, sdUp, exVUp, data, util,
                    nSumers::Int64, Γ::Matrix{Float64},
                    β::Vector{Float64},
                    γ::Float64, α::Float64, BLP::Bool, ζ::Matrix{Float64})

    newSigma = zeros(sigmaSize,2,2)
    newMu = zeros(sigmaSize,2)
    for i in 1:sigmaSize
        invVMap = findall( x->x==i, vMap)

        # println( i)

        if length(invVMap ) == 0
            newSigma[i,:,:] = rand( InverseWishart( 5, [ 5.0  0.0; 0.0  5.0  ])  )
            continue
        end
        
        #println("a")
        sIV = hcat( upsilon[invVMap], xi[invVMap]  )
        #println("b")
        nK = length(invVMap)

        ι = ones(nK);
        yBar = (1/nK)*sIV'*ι

        #println("c")
        muTilde = (nK*yBar + aMu*muBar) / (nK + aMu)
        #println("d")
        sIV -= ι*muTilde'
        #println("e")
        v = 5 + nK
        ## Diffuse prior suggested in Rossi ()
        S = transpose(sIV)*sIV + aMu*(muTilde - muBar)*(muTilde - muBar)'
        #println("f")
        V =  [ 5.0  0.0;
              0.0  5.0  ]

        #println("g")
        ## HAHA funny bug with isposdef() failing on every so slightly non-symmetric matrix?
        newSigma[i,:,:] = rand( InverseWishart( v, Matrix(Symmetric(V+S)))  )
        #println("h")
        newMu[i,1:2] = rand( MvNormal(muTilde,(1.0 / (nK + aMu))*newSigma[i,:,:]))
        #println("i")
        ## Conditional on upsilon, var and exV of ξ (Account for μ here as well)
        sdXi[invVMap] .= sqrt(newSigma[i,2,2] -
                              (newSigma[i,1,2]*newSigma[i,1,2] / newSigma[i,1,1]))
        exVXi[invVMap] = (newSigma[i,1,2] / newSigma[i,1,1])*(upsilon[invVMap] .- newMu[i,1]) .+
            newMu[i,2]
    end

    ## Now we need to update LMarket becuase Sigma and eta have
    ## changed.  Note that warmStart did not change, shares alpha beta
    ## and gamma are all that determine xi, so it remains constant
    ## through this step.
    for t in 1:data.T
        J = data.J[t]
        ξ = util.warmStart[t,1:J] - data.X[t]*β

        if J == 0
            util.LMarket[t] = 0.0
        else
            if BLP
                util.Jac[t] = BuildXiJacBLP(data.X[t], util.warmStart[t,1:J],
                                            nSumers, Γ, ζ, J)
            else
                util.Jac[t] = BuildXiJacBCS(data.X[t], util.solutionFill[t,1:J],
                                            γ, α, J)
            end
            
            util.shareJacLDet[t] = logabsdet(util.Jac[t])[1]
            util.LMarket[t] = MarketLikelihood( ξ, exVXi[data.cMap[t,1:J]],
                                                sdXi[data.cMap[t,1:J]],
                                                J, util.Jac[t])
        end
    end
    return newSigma, newMu, sdXi, exVXi
end

function DoMCMC( data::Data, searchParameters::SearchParameters,
                 M::Int64, ζ::Matrix{Float64}, util::Utility, priors::PriorBLP,
                 lamMap::Vector{Vector{Int64}}, maxMixtures::Int64, burnout::Int64,
                 S::Array{Float64,2}, δ::Array{Array{Float64,1},1}, β::Vector{Float64},
                 η::Vector{Float64}, Γ::Matrix{Float64}, λ::Vector{Float64})

    bigJ = maximum(data.J)


    α = priors.alpha#*1.0 / N

    ## Start with one mixture, let the DP handle it.
    sigmaSize = 1
    vMap = rand(Categorical(ones(1)), data.N);

    V = [1.0 0.0;
         0.0 1.0]
    nu = 5.0

    muBar = priors.muBar
    aMu = priors.aMu


    SkipNum = 2
    
    memAllocateSize = div( M - burnout, SkipNum )
    
    ## Allocate
    π = zeros(memAllocateSize,maxMixtures);
    Σ = zeros(memAllocateSize,maxMixtures,2,2);
    μ = zeros(memAllocateSize,maxMixtures,2);
    betaDraws = zeros(memAllocateSize,data.K);
    etaDraws = zeros(memAllocateSize,data.numInst);
    xi = zeros(memAllocateSize,data.N);
    upsilon = zeros(memAllocateSize,data.N);
    lamT = zeros(memAllocateSize,data.T);
    shareDraws = zeros(memAllocateSize,data.T,bigJ+1);
    gammaDraws = zeros(memAllocateSize,data.K,data.K);

    pMat = zeros(data.T,bigJ);
    xHat = zeros(data.N,data.K);
    zHat = zeros(data.N,data.numInst);

    flatDelta = zeros(data.N);
    flatPrice = zeros(data.N);
    nSumers = 25

    curSigma = zeros(maxMixtures,2,2);
    newSigma = zeros(maxMixtures,2,2);
    curMu = zeros(maxMixtures,2);
    newMu = zeros(maxMixtures,2);
    curBeta = zeros(data.K);
    newBeta = zeros(data.K);
    curEta = zeros(data.numInst);
    newEta = zeros(data.numInst);
    curXi = zeros(data.N);
    newXi = zeros(data.N);
    curUpsilon = zeros(data.N);
    newUpsilon = zeros(data.N);
    curlamT = zeros(data.T);
    newLamT = zeros(data.T);
    curShare = zeros(data.T,bigJ+1);
    newShare = zeros(data.T,bigJ+1);
    curGam = zeros(data.K,data.K);
    newGam = zeros(data.K,data.K);


    
    
    ## Find Starting points
    for i in 1:maxMixtures
        curSigma[i,:,:] = [1.0 0.0; 0.0 1.0]
    end

    ## Without any panel structure to exploit, our starting values will not be so great
    curGam= Γ
    curBeta = β
    curETa = η
    curShare = S
    curlamT = λ
        
    # curBeta = copy(priors.betaMean)
    # for t in 1:data.T
    #     xHat[data.cMap[t,1:data.J[t]],:] = data.X[t]
    #     zHat[data.cMap[t,1:data.J[t]],:] = data.Z[t]
    #     pMat[t,1:data.J[t]] = data.X[t][:,1]
    #     flatPrice[data.cMap[t,1:data.J[t]]] = pMat[t,1:data.J[t]]

    #     if data.searches[t] == 0 || sum( data.q[t,j] for j in 1:data.J[t]) == 0
    #         curShare[t,1:data.J[t]] .= .2 / data.J[t]
    #     else
    #         curShare[t,1:data.J[t]] .= clamp( (sum( data.q[t,j] for j in 1:data.J[t])) /
    #                                               (data.J[t]*data.searches[t]), .05, .5 )
    #         if sum( curShare[t,1:data.J[t]] ) >= 1.0 - 1e-8
    #             curShare[t,1:data.J[t]] ./= sum( curShare[t,1:data.J[t]] )*(2.0)
    #         end            
    #     end
    # end
    # curEta = (zHat'*zHat)\(zHat'*flatPrice)
    # curlamT .= log(mean(data.searches))
    # curGam = diagm( ones( data.K )*.1)#zeros(data.L, data.L)

    #searchParameters = SearchParameters( 0.0001, 0.0005, .5, .75, 0.0);
    saveCount = 1
   

    
    for t in 1:data.T

        xHat[data.cMap[t,1:data.J[t]],:] = data.X[t]
        zHat[data.cMap[t,1:data.J[t]],:] = data.Z[t]
        pMat[t,1:data.J[t]] = data.X[t][:,1]
        flatPrice[data.cMap[t,1:data.J[t]]] = pMat[t,1:data.J[t]]
        
        util.warmStart[t,1:data.J[t]] = δ[t][1:data.J[t]]
            # GInvertShares( data.X[t], curShare[t,1:data.J[t]],
            #                nSumers, curGam, ζ, data.J[t],
        #                zeros(data.J[t]) );

        if PredictSharesBLP( δ[t][1:data.J[t]], data.X[t], 25, Γ, ζ ) != S[t,1:data.J[t]]
            println("t bad: $t")
        end
        
        

        flatDelta[data.cMap[t,1:data.J[t]]] = util.warmStart[t,1:data.J[t]]

        util.Jac[t] = BuildXiJacBLP( data.X[t], util.warmStart[t,1:data.J[t]], nSumers,
                                     curGam, ζ, data.J[t] )

        util.LMarket[t] = MarketLikelihood( util.warmStart[t,1:data.J[t]] -
                                            data.X[t]*curBeta,
                                       zeros(data.J[t]), ones(data.J[t]),
                                       data.J[t], util.Jac[t])
        #println( maximum( abs.( δ[t] - util.warmStart[t,1:data.J[t]])) )
    end

    for t in 1:data.T
        curXi[data.cMap[t,1:data.J[t]]] = util.warmStart[t,1:data.J[t]] -
            data.X[t]*newBeta
        curUpsilon[data.cMap[t,1:data.J[t]]] = pMat[t,1:data.J[t]] -
            data.Z[t]*newEta
    end

    deltaVecHolder = zeros(data.N);
    xHatMod = zeros(data.N);
    sdXi = ones(data.N);
    exVXi = zeros(data.N);
    sdUp = ones(data.N);
    exVUp = zeros(data.N);


    ## 
    for i in 1:sigmaSize
        invVMap = findall( x->x==i, vMap);

        curSigma[i,:,:] = [var(curUpsilon[invVMap]) cov( curUpsilon[invVMap], curXi[invVMap] );
                           cov( curUpsilon[invVMap], curXi[invVMap] ) var(curXi[invVMap])]

        curMu[i,1] = mean( curUpsilon[invVMap])
        curMu[i,2] = mean( curXi[invVMap])

        sdXi[invVMap] .= sqrt(curSigma[i,2,2] -
                              (curSigma[i,1,2]*curSigma[i,1,2] / curSigma[i,1,1]))
        exVXi[invVMap] = (curSigma[i,1,2] / curSigma[i,1,1])*(curUpsilon[invVMap] .-
                                                              curMu[i,1]) .+
                                                              curMu[i,2]

        ## We need the conditional expectations and variances of upsilon too
        sdUp[invVMap] .= sqrt(curSigma[i,1,1] -
                              (curSigma[i,1,2]*curSigma[i,1,2] / curSigma[i,2,2]))
        exVUp[invVMap] = (curSigma[i,1,2] / curSigma[i,2,2])*(curXi[invVMap] .- curMu[i,2]) .+
            curMu[i,1]
    end


    println("Begin Chain")
    for m in 1:(M-1)
        if m % 100 == 0
            println("m = $m, sigmaSize = $sigmaSize")
        end
        

        newLamT = DrawLambda( data, curlamT, newLamT, curShare,
                                  searchParameters, lamMap, priors)

        newShare =  DrawShares( data, curShare, newLamT,
                                curBeta, util, newShare,
                                searchParameters, exVXi, sdXi,
                                data.cMap, nSumers,
                                curGam,
                                0.0, 0.0, true, ζ);



        newGam = DrawGammaBLP( data, util, priors, curGam,
                               exVXi, sdXi, data.cMap,
                               newShare, nSumers,
                               searchParameters, curBeta,
                               ζ)

        
        
        newBeta = DrawBayesReg( data, util.warmStart, deltaVecHolder, util, xHatMod, exVXi, sdXi, data.cMap, xHat, priors.betaMean, priors.betaVarInv, data.K )

        for t in 1:data.T
            newXi[data.cMap[t,1:data.J[t]]] = util.warmStart[t,1:data.J[t]] -
                data.X[t]*newBeta
        end
        
        

        for i in 1:sigmaSize
            invVMap = findall( x->x==i, vMap);
            ## We need the conditional expectations and variances of upsilon too
            sdUp[invVMap] .= sqrt(curSigma[i,1,1] -
                                  (curSigma[i,1,2]*curSigma[i,1,2] / curSigma[i,2,2]))
            exVUp[invVMap] = (curSigma[i,1,2] / curSigma[i,2,2])*(newXi[invVMap] .- curMu[i,2]) .+
                curMu[i,1]
        end
        

        newEta = DrawBayesReg( data, pMat, deltaVecHolder, util, xHatMod, exVUp, sdUp, data.cMap, zHat, priors.etaMean, priors.etaVarInv, data.numInst )

        ## Note that LMarket is no longer valid until DrawSigma changes it.
        for t in 1:data.T
            newUpsilon[data.cMap[t,1:data.J[t]]] = pMat[t,1:data.J[t]] -
                data.Z[t]*newEta
        end
                    
        vMap,curMu,curSigma,sigmaSize = DrawVMapDP( α, sigmaSize, data.N,
                                                    curMu, curSigma,
                                                    newUpsilon, newXi,
                                                    vMap, V, nu,
                                                    aMu, muBar,
                                                    maxMixtures);

       newSigma[1:sigmaSize,:,:], newMu[1:sigmaSize,:], sdXi, exVXi =
            DrawSigma( newUpsilon, newXi, sigmaSize, vMap, muBar, aMu,
                       sdXi, exVXi, sdUp, exVUp, data, util, nSumers,
                       newGam,
                       newBeta, 0.0, 0.0, true, ζ);

        curSigma = copy(newSigma)
        curMu = copy(newMu)
        curBeta = copy(newBeta)
        curEta = copy(newEta)
        curXi = copy(newXi)
        curUpsilon = copy(newUpsilon)
        curlamT = copy(newLamT)
        curShare = copy(newShare)
        curGam = copy(newGam)

        if m > burnout && m % SkipNum == 0
            for n in 1:data.N
                π[saveCount,vMap[n]] += 1.0 / data.N
            end
            Σ[saveCount,1:maxMixtures,1:2,1:2] = newSigma
            μ[saveCount,1:maxMixtures,1:2] = newMu
            betaDraws[saveCount,1:data.K] = newBeta
            etaDraws[saveCount,1:data.numInst] = newEta
            xi[saveCount,1:data.N] = newXi
            upsilon[saveCount,1:data.N] = newUpsilon
            lamT[saveCount,1:data.T] = newLamT
            shareDraws[saveCount,1:data.T,1:(bigJ+1)] = newShare
            gammaDraws[saveCount,1:data.K,1:data.K] = newGam
            saveCount += 1
            
        end

    end
    return betaDraws, etaDraws, μ, Σ, π, xi, upsilon, lamT, shareDraws, gammaDraws
end



